---
title: "Class08"
author: "Yuxuan Jiang PID:A17324184"
format: pdf
toc: true
---
## Background 

We will apply the methods and techniques clustering and PCA to help make sense of a real world breast cancer FNA biopsy data set.

## Exploratory data analysis

### Data import

We syary by improting data. It is a CSV file so we will use the `read.csv()` function.

```{r}
fna.data <- read.csv('WisconsinCancer.csv',row.names = 1)
wisc.df <- data.frame(fna.data)
```

Have a peek of the first few lines:

```{r}
head(wisc.df)
```

Make sure to remove the `diagnosis` column - I don't want to use this for my machine learning models. We will use it later to compare our results to the expert diagnosis. 

```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- wisc.df$diagnosis
```
>Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

>Q2. How many of the observations have a malignant diagnosis?

```{r}
sum(diagnosis=='M')
table(diagnosis)
```

>Q3. How many variables/features in the data are suffixed with _mean?

```{r}
grep('_mean',colnames(wisc.data),value=T)
length(grep('_mean',colnames(wisc.data)))
```

## PCA

The main function is `prcomp()` and we want to make sure we set the optional argument `scale=T`.

```{r}
wisc.pr <- prcomp(wisc.data,scale=T)
summary(wisc.pr)
```
>Q4. From your results, what proportion of the original variance is captured by the first principal component (PC1)?

44.27%

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs:PC1,PC2,PC3

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs:PC 1-7


>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(wisc.pr)
```
The plot is hard to understand as all the points are stacked together and it's hard to identify clustering. 


>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots? 

First PCA figure based on PC1 & PC2:
```{r}
 library(ggplot2)
ggplot(wisc.pr$x)+
  aes(PC1,PC2,col=diagnosis)+
  geom_point()
```

### Variance explained
Calculate the variance of each principal component:
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. 
```{r}
pve <- pr.var / sum(pr.var)
plot(c(1,pve), xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

barplot(pve, ylab = "Percent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```


```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```
### Communicating PCA results
>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC. Are there any features with larger contributions than this one?

```{r}
wisc.pr$rotation[,1]
wisc.pr$rotation['concave.points_mean',1]
```
The feature **concave.points_mean** contributes 26.1% to PC1, which is the largest contributor for PC1 across all the features.

## Hierarchical clustering

>Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist)
plot(wisc.hclust)
abline(h=19,col='red')
```
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust,k=4)
table(wisc.hclust.clusters, diagnosis)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust,k=4)
table(wisc.hclust.clusters, diagnosis)
```

>Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

```{r}
plot(hclust(data.dist,method='single'))
plot(hclust(data.dist,method='complete'))
plot(hclust(data.dist,method='average'))
plot(hclust(data.dist,method='ward.D2'))
```
Method `ward.D2` is the best as it shows the clearest clustering. 

## Combining methods

We will take our PCA results and use those as input for clustering. In other words, our `wisc.pr$x` scores that we plotted above (the main output from PCA - how the data lie on our new principal componet axis/variables) and use a subset of these PCs that capture the most variance as input for `hclust()`.

```{r}
pca.data.dist <- dist(wisc.pr$x[,1:3])
pca.wisc.hclust <- hclust(pca.data.dist,method="ward.D2")
plot(pca.wisc.hclust)
abline(h=80,col='red')
```

Cut the dendrogram into two main groups/clusters:

```{r}
pcawisc.hclust.clusters <- cutree(pca.wisc.hclust,k=2)
table(pcawisc.hclust.clusters)
```

>Q13. How well does the newly created hclust model with two clusters separate out the two “M” and “B” diagnoses?

I want to know how the clustering in `pcawisc.hclust.clusters` with values of 1 or 2 correspond the expert `diagnosis`

```{r}
table(pcawisc.hclust.clusters,diagnosis)
```

By clustering, **group 1** are mostly "M" diagnosis(179) and my clustering **group 2** are mostly "B" diagnosis(333).

24 False Positive(FP)
179 True Positive(TP)
333 True Negative(TN)
33 False Negative(FN)

## Sensitivity/Specificity

>Q14 & Q15. OPTIONAL: Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 6? How do you judge the quality of your result in each case?

**Hierarchical clustering:**
```{r}
table(wisc.hclust.clusters, diagnosis)
```
14 False Positive(FP)
165 True Positive(TP)
343 True Negative(TN)
47 False Negative(FN)
**Sensitivity**:TP/(TP+FN)=165/(165+47)=77.8%
**Specificity**:TN/(TN+FP)=343/(343+14)=96.1%

**Combining methods:**

```{r}
table(pcawisc.hclust.clusters,diagnosis)
```
24 False Positive(FP)
179 True Positive(TP)
333 True Negative(TN)
33 False Negative(FN)
**Sensitivity**:TP/(TP+FN)=179/(179+33)=84.4%
**Specificity**:TN/(TN+FP)=333/(333+24)=93.3%

**Conclusion:** Although hierarchical clustering shows slightly higher **specificity** compared to the combining method, the combined method has a much greater **sensitivity**. Hence, the combining method with PCA is considered the better method in this circumstance.  

## Prediction

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=pcawisc.hclust.clusters)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
>Q16. Which of these new patients should we prioritize for follow up based on your results?

Patients in cluster 2(black points) should be prioritized for follow up.
